{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "x2HFvD8ZHDAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##utilis\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from time import strftime\n",
        "import numpy as np\n",
        "import numpy.linalg as LA\n",
        "import scipy.io as sio\n",
        "import pkg_resources\n",
        "\n",
        "\n",
        "def repmat(A, rows, cols):\n",
        "    return np.tile(A, (cols, rows)).T\n",
        "\n",
        "\n",
        "def vec(A):\n",
        "    # TODO: rewrite docstrings\n",
        "    \"\"\"\n",
        "    * Syntax: `a = vec(A)`\n",
        "    * Vectorization of a matrix. This function is a built-in function in some\n",
        "    recent MATLAB version.\n",
        "    \"\"\"\n",
        "    return A.flatten()\n",
        "\n",
        "\n",
        "def label_to_range(label):\n",
        "    \"\"\"\n",
        "    Convert label to range\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    label: list of integers\n",
        "        must be in the form of [1, 1, ..., 1, 2, 2, ..., 2, ..., C, C, ..., C]\n",
        "        i.e. nondecreasing numbers starting from 1, each element is greater\n",
        "        than the previous element by at most 1\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    a list of intergers with C + 1 elements, start with 0\n",
        "    the i-th element is number of elements in label that equals to i\n",
        "\n",
        "    \"\"\"\n",
        "    res = [0]\n",
        "    # assert label[0] == 1, 'label must start with 1'\n",
        "    for i in range(1, len(label)):\n",
        "        if label[i] == label[i-1]:\n",
        "            continue\n",
        "        if label[i] == label[i-1] + 1:\n",
        "            res.append(i)\n",
        "        else:\n",
        "            assert False,\\\n",
        "                ('label[{}] and label[{}] must be equal or two consecutive '\n",
        "                 'integers, got {} and {}').format(\n",
        "                     i-1, i, label[i-1], label[i]\n",
        "                 )\n",
        "    res.append(len(label))\n",
        "    return res\n",
        "\n",
        "\n",
        "def range_to_label(a_range):\n",
        "    \"\"\"\n",
        "    From a range, convert it to label\n",
        "\n",
        "    This is an inverse function of label_to_range\n",
        "    Parameters:\n",
        "    -----------\n",
        "    a_range: list of integers\n",
        "        must start with 0 and is a strictly increasing list\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "\n",
        "    \"\"\"\n",
        "    assert a_range[0] == 0, 'input must start with 0'\n",
        "    res = []\n",
        "    for i in range(1, len(a_range)):\n",
        "        assert a_range[i] > a_range[i-1],\\\n",
        "            ('a_range must be an increasing list, '\n",
        "             'got a_range[{}] = {} < a_range[{}] = {}').format(\n",
        "                 i, a_range[i], i - 1, a_range[i-1]\n",
        "             )\n",
        "\n",
        "        res.extend([i]*(a_range[i] - a_range[i-1]))\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_block_row(matrix, block_indices, row_range):\n",
        "    \"\"\"\n",
        "    Extract a subset of rows from a matrix\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    matrix: 2-d numpy array\n",
        "        block matrix\n",
        "    block_indices: integer of list of integers\n",
        "        indices of extracted blocks, 0-indexed. If indices is a list, return\n",
        "        the concatenation of all blocks\n",
        "    row_range: list of intergers\n",
        "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
        "        where c_i is the number of rows in the i-th block\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    a 2-d matrix\n",
        "    \"\"\"\n",
        "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
        "    if isinstance(block_indices, int):\n",
        "        block_indices = [block_indices]\n",
        "    # if isinstance(block_indices, (list, np.ndarray, np.generic))\n",
        "    ids = []\n",
        "    for i in block_indices:\n",
        "        ids = ids + list(range(row_range[i], row_range[i+1]))\n",
        "    return matrix[ids, :].copy()\n",
        "\n",
        "\n",
        "def get_block_col(matrix, block_indices, col_range):\n",
        "    \"\"\"\n",
        "    Extract a subset of columns from a matrix\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    matrix: 2-d numpy array\n",
        "        block matrix\n",
        "    block_indices: integer of list of integers\n",
        "        indices of extracted blocks, 1-indexed. If indices is a list, return\n",
        "        the concatenation of all blocks\n",
        "    row_range: list of intergers\n",
        "        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n",
        "        where c_i is the number of columns in the i-th block\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    a 2-d matrix\n",
        "    \"\"\"\n",
        "    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n",
        "    # assert matrix.shape[1] == col_range[-1]\n",
        "    return get_block_row(matrix.T, block_indices, col_range).T\n",
        "\n",
        "\n",
        "def get_block(matrix, i, j, row_range, col_range):\n",
        "    \"\"\"\n",
        "    Extract a submatrix of a matrix\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    matrix the big matrix:\n",
        "    matrix = [ M11, M12, ..., M1m;\n",
        "               M21, M22, ..., M2m;\n",
        "               ... ;\n",
        "               Mn1, Mn2, ..., Mnm]\n",
        "    i: row block index\n",
        "    j: column block index\n",
        "    row_range: row range\n",
        "    col_range: columns range\n",
        "    \"\"\"\n",
        "    return matrix[row_range[i]:row_range[i+1],\n",
        "                  col_range[j]: col_range[j+1]].copy()\n",
        "\n",
        "\n",
        "def norm1(X):\n",
        "    \"\"\"\n",
        "    Return norm 1 of a matrix, which is sum of the absolute value of all elements\n",
        "    of that matrix.\n",
        "    \"\"\"\n",
        "    if X.shape[0]*X.shape[1] == 0:\n",
        "        return 0\n",
        "    return abs(X).sum()\n",
        "\n",
        "\n",
        "def normF2(X):\n",
        "    \"\"\"\n",
        "    Return square of the Frobenius norm, which is sum of square of all\n",
        "    elements in a matrix\n",
        "    \"\"\"\n",
        "    if X.shape[0]*X.shape[1] == 0:\n",
        "        return 0\n",
        "    return LA.norm(X, 'fro')**2\n",
        "\n",
        "\n",
        "def normc(A):\n",
        "    \"\"\"\n",
        "    normalize each column of A to have norm2 = 1\n",
        "    \"\"\"\n",
        "    return A / np.tile(np.sqrt(np.sum(A*A, axis=0)), (A.shape[0], 1))\n",
        "\n",
        "\n",
        "def nuclearnorm(X):\n",
        "    \"\"\"\n",
        "    Return nuclear norm of a matrix.\n",
        "    \"\"\"\n",
        "    if X.size == 0:\n",
        "        return 0\n",
        "    return LA.norm(X) if X.ndim == 1 else LA.norm(X, 'nuc')\n",
        "\n",
        "\n",
        "def shrinkage(U, alambda):\n",
        "    \"\"\"\n",
        "    Soft thresholding function.\n",
        "\n",
        "    Solve the following optimization problem:\n",
        "    X = arg min_X 0.5*||X - U||_F^2 + lambda||X||_1\n",
        "    where U and X are matrices with same sizes. lambda can be either a positive\n",
        "    scalar or a positive matrix (all elements are positive) with same size as X.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, U - alambda) + np.minimum(0, U + alambda)\n",
        "\n",
        "\n",
        "def shrinkage_rank(D, alambda):\n",
        "    \"\"\"\n",
        "    Singular value thresholding algorithm for matrix completion.\n",
        "    Solve the following optimization problem:\n",
        "      X = arg min_X 0.5*||X - D||_F^2 + lambda*||X||_*\n",
        "      where ||X||_* is the nuclear norm.\n",
        "    \"\"\"\n",
        "    U, s, V = LA.svd(D, full_matrices=False)\n",
        "    s1 = np.maximum(0, s - alambda)\n",
        "    return np.dot(U, np.dot(np.diag(s1), V))\n",
        "\n",
        "\n",
        "class MyForm:\n",
        "    \"\"\"\n",
        "    Describe a special family of matrices:\n",
        "    A = [   M 0 ... 0;\n",
        "            0 M ... 0;\n",
        "            0 0 ... M] +\n",
        "        [   N N ... N;\n",
        "            N N ... N;\n",
        "            N N ... N]\n",
        "    with k block rows and columns\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, M, N, k):\n",
        "        self.M = M.copy()\n",
        "        self.N = N.copy()\n",
        "        self.k = k\n",
        "\n",
        "    def full_express(self):\n",
        "        return np.kron(np.eye(self.k), self.M) + np.tile(self.N, (self.k, self.k))\n",
        "\n",
        "    def mult(self, other):\n",
        "        \"\"\"\n",
        "        Multiplication with other MyForm matrix\n",
        "        \"\"\"\n",
        "        A = np.dot(self.M, other.M)\n",
        "        B = np.dot(self.M, other.N) + np.dot(self.N, other.M) + \\\n",
        "            self.k*np.dot(self.N, other.N)\n",
        "        return MyForm(A, B, self.k)\n",
        "\n",
        "    def inv(self):\n",
        "        \"\"\"\n",
        "        compute inverse matrix\n",
        "        \"\"\"\n",
        "        A = LA.inv(self.M)\n",
        "        B = - np.dot(LA.inv(self.M + self.k*self.N), np.dot(self.N, A))\n",
        "        return MyForm(A, B, self.k)\n",
        "\n",
        "    def mult_vec(self, x):\n",
        "        \"\"\"\n",
        "        return M*x (matrix vector multiplication)\n",
        "        \"\"\"\n",
        "        X = x.reshape(self.M.shape[1], self.k, order='F')\n",
        "        p = np.dot(self.N, X).sum(axis=1)\n",
        "\n",
        "        return vec(np.dot(self.M, X)) + vec(np.tile(p, (1, self.k)))\n",
        "\n",
        "\n",
        "def randperm(n):\n",
        "    \"\"\"\n",
        "    get a random permutation of range(n)\n",
        "    \"\"\"\n",
        "    return np.random.permutation(list(range(n)))\n",
        "\n",
        "\n",
        "# def get_range(arange, c):\n",
        "#     return list(range(arange[c], arange[c+1]))\n",
        "\n",
        "\n",
        "def pickDfromY(Y, Y_range, D_range):\n",
        "    \"\"\"\n",
        "    randomly pick k_c samples from Y_c\n",
        "    \"\"\"\n",
        "    C = len(Y_range) - 1\n",
        "    D = np.zeros((Y.shape[0], D_range[-1]))\n",
        "    for c in range(C):\n",
        "        Yc = get_block_col(Y, c, Y_range)\n",
        "        N_c = Yc.shape[1]\n",
        "        # print Yc\n",
        "        ids = randperm(N_c)\n",
        "        kc = D_range[c+1] - D_range[c]\n",
        "        D[:, D_range[c]:D_range[c+1]] = Yc[:, np.sort(ids[:kc])]\n",
        "    return D\n",
        "\n",
        "\n",
        "def load_mat(filename):\n",
        "    return sio.loadmat(filename)\n",
        "\n",
        "\n",
        "def picl_train_test(dataset, N_train_c):\n",
        "    # data_fn = pkg_resources.resource_filename('dictol', 'data/'+dataset + '.mat')\n",
        "    vars_dict = load_mat(\"myYaleB.mat\")\n",
        "    Y = vars_dict['Y']\n",
        "    d = Y.shape[0]\n",
        "    if 'Y_range' not in vars_dict:\n",
        "        Y_range = label_to_range(vars_dict['label'].flatten()).astype(int)\n",
        "\n",
        "    else:\n",
        "        Y_range = vars_dict['Y_range'].flatten().astype(int)\n",
        "    print(Y_range)\n",
        "\n",
        "\n",
        "    C = len(Y_range) - 1\n",
        "    N_total     = Y_range[-1]\n",
        "    N_train     = C*N_train_c\n",
        "    N_test      = N_total - N_train\n",
        "\n",
        "    Y_train     = np.zeros((d, N_train))\n",
        "    Y_test      = np.zeros((d, N_test))\n",
        "    label_train = [0]*N_train\n",
        "    label_test = [0]*N_test\n",
        "    cur_train   = 0\n",
        "    cur_test    = 0\n",
        "    for c in range(C):\n",
        "        Yc        = get_block_col(Y, c, Y_range)\n",
        "        N_total_c = Yc.shape[1]\n",
        "        N_test_c  = N_total_c - N_train_c\n",
        "        label_train[cur_train: cur_train + N_train_c] = [c+1]*N_train_c\n",
        "        label_test[cur_test:cur_test + N_test_c] = [c+1]*N_test_c\n",
        "\n",
        "        ids = randperm(N_total_c)\n",
        "\n",
        "        Y_train[:, cur_train: cur_train + N_train_c] = \\\n",
        "            Yc[:, np.sort(ids[:N_train_c])]\n",
        "\n",
        "        Y_test[:, cur_test: cur_test + N_test_c] = \\\n",
        "            Yc[:, np.sort(ids[N_train_c:])]\n",
        "\n",
        "        cur_train += N_train_c\n",
        "        cur_test += N_test_c\n",
        "\n",
        "    Y_train = normc(Y_train)\n",
        "    Y_test  = normc(Y_test)\n",
        "    return (Y_train, label_train, Y_test, label_test)\n",
        "\n",
        "\n",
        "def range_reduce(D_range, bad_ids):\n",
        "    C = D_range.size - 1\n",
        "    for c in range(C):\n",
        "        cumk = D_range[c+1]\n",
        "        e = cumk - np.nonzero(bad_ids < cumk)[0].size\n",
        "        D_range[c+1] = e\n",
        "\n",
        "\n",
        "# range_reduce_test()\n",
        "def build_mean_vector(X, Y_range):\n",
        "    \"\"\"\n",
        "    M = build_mean_vector(X, Y_range)\n",
        "    suppose X = [X_1 X_2 ... X_C]\n",
        "    return M = [m1, m2, ..., M_C]\n",
        "    where mi = mean(X_i)\n",
        "    \"\"\"\n",
        "    C = Y_range.size -1\n",
        "    M = np.zeros((X.shape[0], C))\n",
        "    for c in range(C):\n",
        "        Xc = get_block_col(X, c, Y_range)\n",
        "        M[:, c] = np.mean(Xc, axis=1)\n",
        "    return M\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def training_test_split(dataset, N_train):\n",
        "    if dataset == 'myARgender':\n",
        "        fn = pkg_resources.resource_filename('dictol', 'data/'+dataset + '.mat')\n",
        "        # fn = os.path.join('data', 'myARgender.pickle')\n",
        "        vars_dict = load_mat(fn)\n",
        "        Y_train = vars_dict['Y_train']\n",
        "        Y_test = vars_dict['Y_test']\n",
        "        label_train = vec(vars_dict['label_train']).astype(int)\n",
        "\n",
        "        label_test = vec(vars_dict['label_test']).astype(int)\n",
        "        range_train = label_to_range(label_train)\n",
        "        # range_test  = label_to_range(label_test)\n",
        "\n",
        "        new_range_train = N_train * np.arange(N_train + 1)\n",
        "        Y_train         = pickDfromY(Y_train, range_train, new_range_train)\n",
        "        label_train     = range_to_label(new_range_train)\n",
        "\n",
        "        Y_train = normc(Y_train)\n",
        "        Y_test  = normc(Y_test)\n",
        "\n",
        "    elif dataset == 'myARreduce':\n",
        "        fn = os.path.join('data', 'AR_EigenFace.pickle')\n",
        "        vars_dict = load_mat(fn)\n",
        "\n",
        "        Y_train     = normc(vars_dict['tr_dat'])\n",
        "        Y_test      = normc(vars_dict['tt_dat'])\n",
        "        label_train = vec(vars_dict['trls']).astype(int)\n",
        "        label_test  = vec(vars_dict['ttls']).astype(int)\n",
        "\n",
        "    elif dataset == 'myFlower':\n",
        "        dataset = 'myFlower102'\n",
        "        # fn      = os.path.join('data', dataset + '.pickle')\n",
        "        vars_dict    = load_mat(\"myFlower102.mat\")\n",
        "\n",
        "        Y_train     = vars_dict['Y_train']\n",
        "        Y_test      = vars_dict['Y_test']\n",
        "        label_train = vec(vars_dict['label_train'])\n",
        "        label_test  = vec(vars_dict['label_test'])\n",
        "        # print(\"label train\", label_train[52000])\n",
        "        range_train = label_to_range(label_train)\n",
        "        print(\"range\", range_train)\n",
        "        num_classes = len(range_train) - 1\n",
        "        new_range_train = N_train * np.arange(num_classes + 1)\n",
        "        label_train = range_to_label(new_range_train)\n",
        "        Y_train = pickDfromY(Y_train, range_train, new_range_train)\n",
        "\n",
        "        Y_train = normc(Y_train)\n",
        "        Y_test = normc(Y_test)\n",
        "\n",
        "    elif dataset == 'mnist':\n",
        "        # dataset = 'myFlower102'\n",
        "        # fn      = os.path.join('data', dataset + '.pickle')\n",
        "        vars_dict    = load_mat(\"mnist-original.mat\")\n",
        "        mnist_data = vars_dict[\"data\"].T\n",
        "        # mnist_data = mnist_data.reshape(len(mnist_data),28,28,1)\n",
        "        mnist_label = vars_dict[\"label\"][0]\n",
        "        # Y_train, Y_test, label_train, label_test = [],[],[],[]\n",
        "        # for i in range(10):\n",
        "        #   Y_train.append(mnist_data[i*7000:(i+1)*7000 - 1000])\n",
        "        #   Y_test.append(mnist_data[(i+1)*7000 - 1000:(i+1)*7000])\n",
        "        #   label_train.append(mnist_label[i*7000:(i+1)*7000 - 1000])\n",
        "        #   label_test.append(mnist_label[(i+1)*7000 - 1000:(i+1)*7000])\n",
        "\n",
        "        # Y_train = np.array(Y_train)\n",
        "        # Y_test = np.array(Y_test)\n",
        "        # label_train = np.array(label_train).flatten()\n",
        "        # label_test = np.array(label_test).flatten()\n",
        "\n",
        "        Y_train, Y_test, label_train, label_test = train_test_split(mnist_data,mnist_label,train_size=60000,test_size=10000,shuffle=False)\n",
        "        print(Y_train.shape)\n",
        "        print(Y_test.shape)\n",
        "        Y_train     = Y_train.T\n",
        "        Y_test      = Y_test.T\n",
        "        print(\"label train\", label_train.shape)\n",
        "        label_train = vec(label_train)\n",
        "        label_test  = vec(label_test)\n",
        "        range_train = label_to_range(label_train)\n",
        "        range_test  = label_to_range(label_test)\n",
        "\n",
        "        print(\"label train\", label_train)\n",
        "        print(\"range\", range_train)\n",
        "        num_classes = len(range_train) - 1\n",
        "        new_range_train = N_train * np.arange(num_classes + 1)\n",
        "        new_range_test = N_train * np.arange(num_classes + 1)\n",
        "        label_train = range_to_label(new_range_train)\n",
        "        label_test = range_to_label(new_range_test)\n",
        "        Y_train = pickDfromY(Y_train, range_train, new_range_train)\n",
        "        Y_test = pickDfromY(Y_test, range_test, new_range_test)\n",
        "        print(\"Y_train\",Y_train)\n",
        "        print(\"Label_train\",label_train)\n",
        "        print(\"Y_test\",Y_test)\n",
        "        print(\"Label_test\",label_test)\n",
        "        Y_train = normc(Y_train)\n",
        "        Y_test = normc(Y_test)\n",
        "    else:\n",
        "        Y_train, label_train, Y_test, label_test = picl_train_test(dataset, N_train)\n",
        "    return (Y_train, Y_test, label_train, label_test)\n",
        "\n",
        "\n",
        "#################### DLCOPAR #######################\n",
        "def buildMhat(M, range_row, range_col):\n",
        "    \"\"\"\n",
        "    buildMhat(M, range_row, range_col):\n",
        "    suppose M = [M11 M12 ... M1n;\n",
        "                      M21 M22 ... M3n;\n",
        "                      .....\n",
        "                      Mn1 Mn2 .... Mnn]\n",
        "        then Mhat = = [2*M11  M12     ... M1n;\n",
        "                       M21    2*M22   ... M3n;\n",
        "                          .....\n",
        "                       Mn1     Mn2 .... 2*Mnn]\n",
        "    ---------------------------------------------\n",
        "    Author: Tiep Vu, thv102@psu.edu, 04/21/2016\n",
        "            http://www.personal.psu.edu/thv102/\n",
        "    ---------------------------------------------\n",
        "    \"\"\"\n",
        "    C = len(range_row) - 1\n",
        "    M2 = M.copy()\n",
        "    for c in range(C):\n",
        "        M2[range_row[c]: range_row[c+1], range_col[c]: range_col[c+1]] *= 2\n",
        "    return M2\n",
        "\n",
        "\n",
        "def buildM_2Mbar(X, Y_range, lambda2):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    MM = np.zeros_like(X)\n",
        "    C = len(Y_range) - 1\n",
        "    m = np.mean(X, axis=1)\n",
        "    for c in range(C):\n",
        "        Xc = get_block_col(X, c, Y_range)\n",
        "        mc = np.mean(Xc, axis=1)\n",
        "        MM[:, Y_range[c]: Y_range[c+1]] = repmat(lambda2*(m - 2*mc), 1, Y_range[c+1] - Y_range[c])\n",
        "    return MM\n",
        "\n",
        "\n",
        "def build_mean_matrix(X, cols=None):\n",
        "    \"\"\"\n",
        "    repeat np.mean(X, axis = 1) cols times.\n",
        "\n",
        "    ---------------\n",
        "    Parameters:\n",
        "    X: 2d numpy array\n",
        "    cols: int\n",
        "        if cols == None then cols = #cols of X\n",
        "\n",
        "    \"\"\"\n",
        "    if len(X.shape) < 2 or X.shape[1] == 0:\n",
        "        return X\n",
        "    mean_vector = np.mean(X, axis=1)\n",
        "    if cols is None:\n",
        "        return repmat(mean_vector, 1, X.shape[1])\n",
        "    else:\n",
        "        return np.tile(mean_vector, cols)\n",
        "\n",
        "\n",
        "def range_delete_ids(a_range, ids):\n",
        "    \"\"\"\n",
        "    given a range a_range of an array. Suppose we want to delete some\n",
        "    element of that array indexed by `ids`, `new_range` is the new range\n",
        "    \"\"\"\n",
        "    ids = np.sort(ids)\n",
        "    n = a_range.size\n",
        "    a = np.zeros_like(a_range)\n",
        "    j = 1\n",
        "    while j < n-1:\n",
        "        for i in range(n):\n",
        "            while a_range[j] < ids[i]:\n",
        "                j += 1\n",
        "            for k in range(j, n):\n",
        "                a[k] += 1\n",
        "\n",
        "    new_range = a_range - a\n",
        "    return new_range\n",
        "\n",
        "\n",
        "def range_delete_ids_test():\n",
        "    a_range = np.array([0, 3, 5, 10])\n",
        "    ids = np.array([1, 4, 7, 10])\n",
        "    print(a_range)\n",
        "    print(range_delete_ids(a_range, ids))\n",
        "\n",
        "\n",
        "def max_eig(D):\n",
        "    \"\"\"\n",
        "    return maximum eigenvalue of matrix D\n",
        "    \"\"\"\n",
        "    return np.max(LA.eig(D)[0])\n",
        "\n",
        "\n",
        "def erase_diagonal(A):\n",
        "    if A.shape[0] != A.shape[1]:\n",
        "        print('The input matrix is not square!')\n",
        "        return\n",
        "    B = A.copy()\n",
        "    np.fill_diagonal(B, 0)\n",
        "    return B\n",
        "\n",
        "\n",
        "def erase_diagonal_blocks(A, row_range, col_range):\n",
        "    \"\"\" remove diagonal blocks of the block matrix A \"\"\"\n",
        "    if len(row_range) != len(col_range):\n",
        "        print('no. of column blocks != no. of row blocks!!')\n",
        "    C = len(row_range) - 1\n",
        "    B = A.copy()\n",
        "    for c in range(C):\n",
        "        B[row_range[c]: row_range[c+1], col_range[c]: col_range[c+1]] = 0\n",
        "    return B\n",
        "\n",
        "\n",
        "def inv_IpXY(X, Y):\n",
        "    \"\"\"\n",
        "    Calculate the inverse of matrix A = I + XY.\n",
        "    if X is a fat matrix (number of columns >> number of rows), then use inv(I + X*Y)\n",
        "    else: use equation: (I + XY)^(-1) = I - Y*(I + Y*X)^(-1)*X\n",
        "    \"\"\"\n",
        "    d1 = X.shape[0]\n",
        "    d2 = X.shape[1]\n",
        "    if d1 > d2:\n",
        "        M = np.eye(d1) - np.dot(np.dot(X, LA.inv(np.eye(d2) + np.dot(Y, X))), Y)\n",
        "    else:\n",
        "        M = LA.inv(np.eye(d1) + np.dot(X, Y))\n",
        "    return M\n",
        "\n",
        "\n",
        "def progress_str(cur_val, max_val, total_point=50):\n",
        "    p = int(math.ceil(float(cur_val)*total_point / max_val))\n",
        "    return '|' + p*'#' + (total_point - p)*'.' + '|'\n",
        "\n",
        "\n",
        "def get_time_str():\n",
        "    print('Time now: ' + strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
        "    return strftime(\"%m%d_%H%M%S\")\n"
      ],
      "metadata": {
        "id": "nbmyQZTPnJMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###optimise\n",
        "def ODL_updateD(D, E, F, iterations=100, tol=1e-8):\n",
        "    \"\"\"\n",
        "    The main algorithm in ODL.\n",
        "    Solving the optimization problem:\n",
        "      D = arg min_D -2trace(E'*D) + trace(D*F*D') subject to: ||d_i||_2 <= 1,\n",
        "         where F is a positive semidefinite matrix.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    D, E, F as in the above problem.\n",
        "    iterations: maximum number of iterations.\n",
        "    tol: when the difference of solutions in two successive\n",
        "        iterations less than this value, the algorithm will stop.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    \"\"\"\n",
        "    def calc_cost(D):\n",
        "        return -2*np.trace(np.dot(E, D.T)) + np.trace(np.dot(np.dot(F, D.T), D))\n",
        "\n",
        "    D_old = D.copy()\n",
        "    for _ in range(iterations):\n",
        "        for i in range(D.shape[1]):\n",
        "            if F[i, i] != 0:\n",
        "                a = 1.0/F[i, i] * (E[:, i] - D.dot(F[:, i])) + D[:, i]\n",
        "                D[:, i] = a/max(LA.norm(a, 2), 1)\n",
        "\n",
        "        if LA.norm(D - D_old, 'fro')/D.size < tol:\n",
        "            break\n",
        "        D_old = D.copy()\n",
        "    return D\n",
        "\n",
        "\n",
        "def DLSI_updateD(D, E, F, A, lambda1, iterations=100):\n",
        "    \"\"\"\n",
        "    def DLSI_updateD(D, E, F, A, lambda1, verbose = False, iterations = 100):\n",
        "    problem: `D = argmin_D -2trace(ED') + trace(FD'*D) + lambda *||A*D||F^2,`\n",
        "    subject to: `||d_i||_2^2 <= 1`\n",
        "    where F is a positive semidefinite matrix\n",
        "    ========= aproach: ADMM ==============================\n",
        "    rewrite: `[D, Z] = argmin -2trace(ED') + trace(FD'*D) + lambda ||A*Z||_F^2,`\n",
        "        subject to `D = Z; ||d_i||_2^2 <= 1`\n",
        "    aproach 1: ADMM.\n",
        "    1. D = -2trace(ED') + trace(FD'*D) + rho/2 ||D - Z + U||_F^2,\n",
        "        s.t. ||d_i||_2^2 <= 1\n",
        "    2. Z = argmin lambda*||A*Z|| + rho/2||D - Z + U||_F^2\n",
        "    3. U = U + D - Z\n",
        "    solve 1: D = argmin -2trace(ED') + trace(FD'*D) + rho/2 ||D - W||_F^2\n",
        "                          with W = Z - U;\n",
        "               = argmin -2trace((E - rho/2*W)*D') +\n",
        "                  trace((F + rho/2 * eye())*D'D)\n",
        "    solve 2: derivetaive: 0 = 2A'AZ + rho (Z - V) with V = D + U\n",
        "    `Z = B*rhoV` with `B = (2*lambda*A'*A + rho I)^{-1}`\n",
        "    `U = U + D - Z`\n",
        "    -----------------------------------------------\n",
        "    \"\"\"\n",
        "    def calc_cost(D):\n",
        "        cost = -2*np.trace(np.dot(E, D.T)) + np.trace(np.dot(F, np.dot(D.T, D))) +\\\n",
        "            lambda1*normF2(np.dot(A, D))\n",
        "        return cost\n",
        "    it = 0\n",
        "    rho = 1.0\n",
        "    Z_old = D.copy()\n",
        "    U = np.zeros_like(D)\n",
        "    I_k = np.eye(D.shape[1])\n",
        "    X = 2*lambda1/rho*A.T\n",
        "    Y = A.copy()\n",
        "    B1 = np.dot(X, inv_IpXY(Y, X))\n",
        "\n",
        "    # B1 = np.dot(X, LA.inv(eye(Y.shape[0]) + np.dot(Y, X)))\n",
        "    tol = 1e-8\n",
        "    for it in range(iterations):\n",
        "        it += 1\n",
        "        # update D\n",
        "        W  = Z_old - U\n",
        "        E2 = E + rho/2*W\n",
        "        F2 = F + rho/2*I_k\n",
        "        D  = ODL_updateD(D, E2, F2)\n",
        "        # update Z\n",
        "        V = D + U\n",
        "        Z_new = rho*(V - np.dot(B1, np.dot(Y, V)))\n",
        "        e1 = normF2(D - Z_new)\n",
        "        e2 = rho*normF2(Z_new - Z_old)\n",
        "        if e1 < tol and e2 < tol:\n",
        "            break\n",
        "        U = U + D - Z_new\n",
        "        Z_old = Z_new.copy()\n",
        "\n",
        "    return D\n",
        "\n",
        "\n",
        "def num_grad(func, X):\n",
        "    \"\"\"\n",
        "    Calculating gradient of a function `func(X)` where `X` is a matrix or\n",
        "    vector\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(X)\n",
        "    eps = 1e-4\n",
        "    # TODO: flatten then unflatten, make it independent on X.shape\n",
        "    # the current implementation only work with 2-d array\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            # print X, '\\n'\n",
        "            Xp = X.copy()\n",
        "            Xm = X.copy()\n",
        "            Xp[i, j] += eps\n",
        "            # print X\n",
        "            fp = func(Xp)\n",
        "            Xm[i, j] -= eps\n",
        "            fm = func(Xm)\n",
        "            grad[i, j] = (fp - fm)/(2*eps)\n",
        "    return grad\n",
        "\n",
        "\n",
        "def check_grad(func, grad, X):\n",
        "    print('Checking grad...',)\n",
        "    grad1 = grad(X)\n",
        "    grad2 = num_grad(func, X)\n",
        "\n",
        "    dif =  LA.norm(grad1 - grad2)\n",
        "    if dif < 1e-5:\n",
        "        print('Different = %f' %dif, 'PASS')\n",
        "    else:\n",
        "        print('Different = %f' %dif, 'FAIL')\n",
        "    return dif < 1e-5\n",
        "\n",
        "\n",
        "def min_rank_dict(Y, X, lambdaD, Dinit, iterations = 100, tol = 1e-8):\n",
        "    \"\"\"\n",
        "    [D] = argmin_D 0.5*|| Y - DX||_F^2 + lambdaD ||D||_*\n",
        "    s.t. ||d_i||_2^2 <= 1, for all i\n",
        "    using ADMM:\n",
        "    INPUT:\n",
        "        Y: Data\n",
        "        Dinit: intitial D\n",
        "        X: sparse code\n",
        "        lambdaD: regularization term\n",
        "    OUTPUT:\n",
        "        D:\n",
        "    Created: Tiep Vu 6/29/2015 2:05:28 PM\n",
        "    ------------------------\n",
        "    Choose a rho.\n",
        "    Algorithm summary\n",
        "    ADMM: D,J = argmin_DJ 0.5*||Y - DX||_F^2 + lambdaD||J||_*\n",
        "    s.t ||d_i||_2^2 <= 1 and J = D\n",
        "    Alternatively solving:\n",
        "    (1): D^{k+1} = argmin_D 0.5*||Y - DX||_F^2 + rho/2 ||J - D + U^k||_F^2\n",
        "        s.t. ||d_i||_2^2 <= 1\n",
        "        this problem can be soved using the update dictionary stage in\n",
        "            Online Dictionary Learning method\n",
        "    (2): J^{k+1} = argminJ lambdaD||J||_* + rho/2||J - D^{k+1} + U^k||\n",
        "        Solution: shrinkage_rank(D^{k+1} - U^k, lambdaD/rho)\n",
        "    (3): Update U: U^{k+1} = U^k + J^{k+1} - D^{k+1}\n",
        "     Stoping cretia:\n",
        "    ||r^k||_F^2 <= tol, ||s^k||_F^2 <= tol\n",
        "    r^k = J^k - D^k\n",
        "    s^k = rho(J^{k+1} - J^k)\n",
        "    ---------------------------------------------\n",
        "    Author: Tiep Vu, thv102@psu.edu, 04/22/2016\n",
        "            http://www.personal.psu.edu/thv102/\n",
        "    ---------------------------------------------\n",
        "    \"\"\"\n",
        "    YXt = np.dot(Y, X.T)\n",
        "    XXt = np.dot(X, X.T)\n",
        "    rho = 0.25\n",
        "    D_old = Dinit\n",
        "    J_old = Dinit\n",
        "    U_old = np.zeros_like(Dinit)\n",
        "    it = 0\n",
        "    I = np.eye(XXt.shape[0])\n",
        "    tau = 2\n",
        "    mu = 10.0\n",
        "    for it in range(iterations):\n",
        "        ## =========update D ================================\n",
        "        # D = argmin_D 0.5*||Y - DX||_F^2 + rho/2 ||J - D + U||_F^2\n",
        "        # s.t. ||d_i||_2^2 <= 1\n",
        "        E = YXt + rho*(J_old + U_old)\n",
        "        F = XXt + rho*I\n",
        "        # D_new = updateD_EF(D_old, E, F, 10);\n",
        "        D_new = ODL_updateD(D_old, E, F, iterations = 30)\n",
        "        ## ========= update J ==============================\n",
        "        # J^{k+1} = argminJ lambdaD||J||_* + rho/2||J - D + U||\n",
        "        J_new = np.real(shrinkage_rank(D_old - U_old, lambdaD/rho))\n",
        "         ## ========= update U ==============================\n",
        "        U_new = U_old + J_new - D_old\n",
        "        ## ========= check stop ==============================\n",
        "        r = J_new - D_old\n",
        "        s = rho*(J_new - J_old)\n",
        "        r_eps = LA.norm(r, 'fro')\n",
        "        s_eps = LA.norm(s, 'fro')\n",
        "        if r_eps < tol and s_eps < tol:\n",
        "            break\n",
        "        D_old = D_new\n",
        "        J_old = J_new\n",
        "        U_old = U_new\n",
        "        if r_eps > mu*s_eps:\n",
        "            rho = rho*tau\n",
        "        elif s_eps > mu*r_eps:\n",
        "            rho = rho/tau\n",
        "    return D_new\n",
        "\n",
        "\n",
        "class Fista(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        subclasses are required to have three following functions and lambd\n",
        "        \"\"\"\n",
        "        self._grad = None\n",
        "        self._calc_f = None\n",
        "        self.lossF = None\n",
        "        self.lambd = None\n",
        "        self.D = None\n",
        "        self.Y = None\n",
        "        self.L = None\n",
        "\n",
        "    def solve(self, Xinit=None, iterations=100, tol=1e-8, verbose=False):\n",
        "        if Xinit is None:\n",
        "            Xinit = np.zeros((self.D.shape[1], self.Y.shape[1]))\n",
        "        Linv = 1/self.L\n",
        "        lambdaLiv = self.lambd/self.L\n",
        "        x_old = Xinit.copy()\n",
        "        y_old = Xinit.copy()\n",
        "        t_old = 1\n",
        "        it = 0\n",
        "        # cost_old = float(\"inf\")\n",
        "        for it in range(iterations):\n",
        "            x_new = np.real(shrinkage(y_old - Linv*self._grad(y_old), lambdaLiv))\n",
        "            t_new = .5*(1 + math.sqrt(1 + 4*t_old**2))\n",
        "            y_new = x_new + (t_old - 1)/t_new * (x_new - x_old)\n",
        "            e = norm1(x_new - x_old)/x_new.size\n",
        "            if e < tol:\n",
        "                break\n",
        "            x_old = x_new.copy()\n",
        "            t_old = t_new\n",
        "            y_old = y_new.copy()\n",
        "            if verbose:\n",
        "                print('iter \\t%d/%d, loss \\t %4.4f'%(it + 1, iterations, self.lossF(x_new)))\n",
        "        return x_new\n",
        "\n",
        "    def _grad(self, y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def lossF(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def check_grad(self, X):\n",
        "        grad1 = self._grad(X)\n",
        "        grad2 = num_grad(self._calc_f, X)\n",
        "        dif = norm1(grad1 - grad2)/grad1.size\n",
        "        print('grad difference = %.7f'%dif)\n",
        "\n",
        "\n",
        "class Lasso(Fista):\n",
        "    \"\"\"\n",
        "    Solving a Lasso optimization problem using FISTA\n",
        "    `X, = arg min_X 0.5*||Y - DX||_F^2 + lambd||X||_1\n",
        "        = argmin_X f(X) + lambd||X||_1\n",
        "        F(x) = f(X) + lamb||X||_1\n",
        "    \"\"\"\n",
        "    def __init__(self, D, lambd = .1):\n",
        "        self.D = D\n",
        "        self.lambd = lambd\n",
        "        self.DtD = np.dot(self.D.T, self.D)\n",
        "        self.Y = None\n",
        "        self.DtY = None\n",
        "        self.L = np.max(LA.eig(self.DtD)[0])\n",
        "        self.coef_ = None\n",
        "\n",
        "    def fit(self, Y, Xinit = None, iterations = 100):\n",
        "        self.Y = Y\n",
        "        self.DtY = np.dot(self.D.T, self.Y)\n",
        "        if Xinit is None:\n",
        "            Xinit = np.zeros((self.D.shape[1], self.Y.shape[1]))\n",
        "        self.coef_ = self.solve(Xinit=Xinit, iterations=iterations)\n",
        "\n",
        "    def _grad(self, X):\n",
        "        return np.dot(self.DtD, X) - self.DtY\n",
        "\n",
        "    def _calc_f(self, X):\n",
        "        return 0.5*normF2(self.Y - np.dot(self.D, X))\n",
        "\n",
        "    def lossF(self, X):\n",
        "        return self._calc_f(X) + self.lambd*norm1(X)"
      ],
      "metadata": {
        "id": "W4PRCN6qnVqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Odl\n",
        "class ODL(object):\n",
        "    \"\"\"\n",
        "    Solving the optimization problem:\n",
        "        (D, X) = arg min_{D, X} 0.5||Y - DX||_F^2 + lamb||X||_1\n",
        "    \"\"\"\n",
        "    def __init__(self, k, lambd=0.001, updateD_iters=100, updateX_iters=100):\n",
        "        self.lambd = lambd\n",
        "        self.k = k\n",
        "        self.Y = None\n",
        "        self.D = None\n",
        "        self.X = None\n",
        "        self.updateD_iters = updateD_iters\n",
        "        self.updateX_iters = updateX_iters\n",
        "\n",
        "    def fit(self, Y, iterations=100, verbose=False):\n",
        "        \"\"\"\n",
        "        Y: numpy data [n_features, n_samples]\n",
        "        k: interger: number of atoms in the dictionary\n",
        "            if k is None, select k = round(0.2*n_samples)\n",
        "        \"\"\"\n",
        "        self.Y = Y\n",
        "        Y_range = np.array([0, self.Y.shape[1]])\n",
        "        D_range = np.array([0, self.k])\n",
        "        self.D = pickDfromY(self.Y, Y_range, D_range)\n",
        "        self.X = np.zeros((self.D.shape[1], self.Y.shape[1]))\n",
        "        for it in range(iterations):\n",
        "            # update X\n",
        "            lasso = Lasso(self.D, self.lambd)\n",
        "            lasso.fit(self.Y, Xinit = self.X)\n",
        "            self.X = lasso.coef_\n",
        "            # update D\n",
        "            F = np.dot(self.X, self.X.T)\n",
        "            E = np.dot(self.Y, self.X.T)\n",
        "            self.D = ODL_updateD(self.D, E, F, iterations = self.updateD_iters)\n",
        "            if verbose:\n",
        "                print('iter \\t%d/%d \\t\\t loss \\t%.4f'%(it, iterations, self.loss()))\n",
        "\n",
        "    def loss(self):\n",
        "        loss = 0.5*normF2(self.Y - np.dot(self.D, self.X)) + \\\n",
        "                self.lambd*norm1(self.X)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "o7ihzRA9nCd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### base\n",
        "class BaseModel(object):\n",
        "    \"\"\"\n",
        "    base dictionary learning model for classification\n",
        "    \"\"\"\n",
        "    # def __init__(self)\n",
        "    def predict(self, Y):\n",
        "        N = Y.shape[1]\n",
        "        E = np.zeros((self.num_classes, N))\n",
        "        for c in range(self.num_classes):\n",
        "            # Dc in D only\n",
        "            Dc_ = get_block_col(self.D, c, self.D_range)\n",
        "            # Dc in D and D0\n",
        "            Dc = np.hstack((Dc_, self.D0)) if self.k0 > 0 else Dc_\n",
        "            lasso = Lasso(Dc, lambd=self.lambd)\n",
        "            lasso.fit(Y)\n",
        "            Xc = lasso.solve()\n",
        "            residual_matrix = Y - np.dot(Dc, Xc)\n",
        "            E[c, :] = 0.5*np.sum(residual_matrix*residual_matrix, axis=0) +\\\n",
        "                self.lambd*np.sum(np.abs(Xc), axis=0)\n",
        "        pred = np.argmin(E, axis=0) + 1\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def evaluate(self, data, label):\n",
        "        pred = self.predict(data)\n",
        "        acc = np.sum(pred == label)/float(len(label))\n",
        "        print('accuracy = {:.2f} %'.format(100 * acc))\n",
        "        return acc\n"
      ],
      "metadata": {
        "id": "1dbHPFv1ntDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "kXx-xLp-UEE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq4q_lCihkMi",
        "outputId": "cf1d21d3-86e7-435a-e2d1-efc00fc07d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================\n",
            "Unit test: Low-rank shared Dictionary Learning\n",
            "(60000, 784)\n",
            "(10000, 784)\n",
            "label train (60000,)\n",
            "label train [0. 0. 0. ... 9. 9. 9.]\n",
            "range [0, 5923, 12665, 18623, 24754, 30596, 36017, 41935, 48200, 54051, 60000]\n",
            "Y_train [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Label_train [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
            "Y_test [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Label_test [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
            "initializing ... \n",
            "initial loss 675.4415\n",
            "iter \t  1/ 40 \t loss 228.5412\n",
            "iter \t  5/ 40 \t loss 201.7815\n",
            "iter \t 10/ 40 \t loss 195.3664\n",
            "iter \t 15/ 40 \t loss 191.5464\n",
            "iter \t 20/ 40 \t loss 189.1290\n",
            "iter \t 25/ 40 \t loss 187.5321\n",
            "iter \t 30/ 40 \t loss 186.4799\n",
            "iter \t 35/ 40 \t loss 185.8049\n",
            "iter \t 40/ 40 \t loss 185.3433\n",
            "Dc_sum: 183.3410277582555\n",
            "sparsity 3750\n",
            "Dc_sum: 128.1146970611433\n",
            "sparsity 3750\n",
            "Dc_sum: 163.36823837531387\n",
            "sparsity 3750\n",
            "Dc_sum: 148.76055471290013\n",
            "sparsity 3750\n",
            "Dc_sum: 143.80083810937552\n",
            "sparsity 3750\n",
            "Dc_sum: 142.39762838473996\n",
            "sparsity 3750\n",
            "Dc_sum: 153.52137189655372\n",
            "sparsity 3750\n",
            "Dc_sum: 136.298105575867\n",
            "sparsity 3750\n",
            "Dc_sum: 148.97534326997337\n",
            "sparsity 3750\n",
            "Dc_sum: 143.41927684592116\n",
            "sparsity 3750\n",
            "accuracy = 92.40 %\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Low-rank Shared Dictionary Learning\n",
        "and Fisher Discriminant Dictionary Learning\n",
        "\"\"\"\n",
        "\n",
        "# from __future__ import print_function\n",
        "# from . import optimize\n",
        "# from . import utils, base\n",
        "# from .utils import normF2, norm1, get_block_col, get_block_row, nuclearnorm, build_mean_matrix\n",
        "# import numpy as np\n",
        "# from .ODL import ODL\n",
        "\n",
        "_zero = np.array([0])\n",
        "\n",
        "class _UpdateXX0(Fista):\n",
        "    \"\"\"\n",
        "    solve XX0 in LRSDL using Fista\n",
        "    \"\"\"\n",
        "    def __init__(self, Y, Y_range, D, D_range, D0, k0, lambd=0.01, lambd2=0.01):\n",
        "        self.Y = Y\n",
        "        self.Y_range = Y_range\n",
        "        self.num_classes = len(Y_range) - 1\n",
        "        self.D = D\n",
        "        self.D_range = D_range\n",
        "        self.D0 = D0\n",
        "        self.k0 = k0\n",
        "        self.lambd = lambd\n",
        "        self.lambd2 = lambd2\n",
        "        self.DtD = np.dot(D.T, D)\n",
        "        self.D_0 = buildMhat(self.DtD, self.D_range, self.D_range)\n",
        "        self.Dhat = self.D_0 + 2*self.lambd2*np.eye(self.D_0.shape[1])\n",
        "        self.D0tD0 = np.dot(self.D0.T, self.D0) if self.k0 > 0 else _zero\n",
        "        self.A = 2*self.D0tD0 + self.lambd2*np.eye(self.k0) if self.k0 > 0 else _zero\n",
        "        self.DtD0 = np.dot(D.T, D0) if self.k0 > 0 else _zero\n",
        "        self.D0tD = self.DtD0.T\n",
        "        self.D0tY2 = 2*np.dot(self.D0.T, self.Y) if self.k0 > 0 else _zero\n",
        "\n",
        "        self.DtY0 = np.dot(self.D.T, self.Y)\n",
        "        self.L = max_eig(self.Dhat) + 4*self.lambd2 + 1\n",
        "        if self.k0 > 0:\n",
        "            self.L += max_eig(self.A)\n",
        "\n",
        "    def _extract_fromX1(self, X1):\n",
        "        K = self.D_range[-1]\n",
        "        return (X1[:K, :], X1[K:, :]) if self.k0 > 0 else (X1[:K, :], _zero)\n",
        "\n",
        "    def _grad(self, X1):\n",
        "        X, X0 = self._extract_fromX1(X1)\n",
        "        DtY = self.DtY0 - np.dot(self.DtD0, X0)\n",
        "        Y_0 = buildMhat(DtY, self.D_range, self.Y_range)\n",
        "        g = np.dot(self.Dhat, X) - Y_0 + buildM_2Mbar(X, self.Y_range, self.lambd2)\n",
        "        if self.k0 > 0:\n",
        "            g0 = np.dot(self.A, X0) - self.D0tY2 +\\\n",
        "                np.dot(self.D0tD, buildMhat(X, self.D_range, self.Y_range)) -\\\n",
        "                self.lambd2*build_mean_matrix(X0)\n",
        "            return np.vstack((g, g0))\n",
        "        return g\n",
        "\n",
        "    def _fidelity(self, X):\n",
        "        \"\"\"\n",
        "        * Calculating the fidelity term in FDDL[[4]](#fn_fdd):\n",
        "        * $\\sum_{c=1}^C \\Big(\\|Y_c - D_cX^c_c\\|_F^2 +\n",
        "            \\sum_{i \\neq c} \\|D_c X^c_i\\|_F^2\\Big)$\n",
        "        \"\"\"\n",
        "        cost = 0\n",
        "        Y = self.Y\n",
        "        for c in range(self.num_classes):\n",
        "            Yc = get_block_col(Y, c, self.Y_range)\n",
        "            Dc = get_block_col(self.D, c, self.D_range)\n",
        "            Xc = get_block_row(X, c, self.D_range)\n",
        "            Xcc = get_block_col(Xc, c, self.Y_range)\n",
        "            cost += normF2(Yc - np.dot(Dc, Xcc))\n",
        "            for i in range(self.num_classes):\n",
        "                if i == c:\n",
        "                    continue\n",
        "                Xci = get_block_col(Xc, i, self.Y_range)\n",
        "                cost += normF2(np.dot(Dc, Xci))\n",
        "        return cost\n",
        "\n",
        "    def _discriminative(self, X):\n",
        "        \"\"\"\n",
        "        * calculating the discriminative term in\n",
        "        * $\\|X\\|_F^2 + \\sum_{c=1}^C (\\|Xc - Mc\\|_F^2 - \\|Mc - M\\|_F^2) $\n",
        "        \"\"\"\n",
        "        cost = normF2(X)\n",
        "        m = np.mean(X, axis=1)\n",
        "        for c in range(self.num_classes):\n",
        "            Xc = get_block_col(X, c, self.Y_range)\n",
        "            Mc = build_mean_matrix(Xc)\n",
        "            cost += normF2(Xc - Mc)\n",
        "            M = repmat(m, 1, Xc.shape[1])\n",
        "            cost -= normF2(Mc - M)\n",
        "        return cost\n",
        "\n",
        "    def _calc_f(self, X1):\n",
        "        X, X0 = self._extract_fromX1(X1)\n",
        "        Ybar = self.Y - np.dot(self.D0, X0)\n",
        "        cost = 0.5*(normF2(Ybar - np.dot(self.D, X)) + self._fidelity(X)) + \\\n",
        "            0.5*self.lambd2*self._discriminative(X) + normF2(X0 - build_mean_matrix(X0))\n",
        "        return cost\n",
        "\n",
        "    def lossF(self, X1):\n",
        "        return self._calc_f(X1) + self.lambd*norm1(X1)\n",
        "\n",
        "\n",
        "class LRSDL(BaseModel):\n",
        "    def __init__(self, lambd=0.01, lambd2=0.01, eta=0.0001,\n",
        "            k=10, k0=5, updateX_iters=100, updateD_iters=100):\n",
        "        self.lambd = lambd\n",
        "        self.lambd2 = lambd2\n",
        "        self.eta = eta\n",
        "        self.D = None\n",
        "        self.X = None\n",
        "        self.Y = None\n",
        "        self.k = k\n",
        "        self.k0 = k0\n",
        "        self.updateX_iters = updateX_iters\n",
        "        self.updateD_iters = updateD_iters\n",
        "        self.D_range = None\n",
        "        self.D0 = None\n",
        "        self.Y_range = None\n",
        "        self.X = None\n",
        "        self.X0 = None\n",
        "\n",
        "    def _getYc(self, c):\n",
        "        return get_block_col(self.Y, c, self.Y_range)\n",
        "\n",
        "    def fit(self, Y, train_label, verbose=False, iterations=100, show_after=5):\n",
        "        self.Y_range = label_to_range(train_label)\n",
        "        self.num_classes = len(self.Y_range) - 1\n",
        "        self.D_range = [self.k*i for i in range(self.num_classes+1)]\n",
        "        self.Y = Y\n",
        "        self.D = np.zeros((self.Y.shape[0], self.D_range[-1]))\n",
        "        self.X = np.zeros((self.D_range[-1], self.Y.shape[1]))\n",
        "        if self.k0 > 0:\n",
        "            self.D0 = np.zeros((self.Y.shape[0], self.k0))\n",
        "            self.X0 = np.zeros((self.k0, self.Y.shape[1]))\n",
        "\n",
        "        # init\n",
        "        if verbose:\n",
        "            print('initializing ... ')\n",
        "        self._initialize()\n",
        "        if verbose:\n",
        "            print('initial loss %.4f' % self.loss())\n",
        "\n",
        "        # train\n",
        "        for it in range(iterations):\n",
        "            # update D\n",
        "            self._updateD()\n",
        "            # update D0\n",
        "            if self.k0 > 0:\n",
        "                self._updateD0()\n",
        "            self._updateXX0()\n",
        "            if verbose and (it == 0 or (it + 1) % show_after == 0):\n",
        "                print('iter \\t%3d/%3d \\t loss %.4f' % (it+1, iterations, self.loss()))\n",
        "\n",
        "    def _updateD(self):\n",
        "        Y = self.Y if self.k0 == 0 else self.Y - np.dot(self.D0, self.X0)\n",
        "        F = buildMhat(np.dot(self.X, self.X.T), self.D_range, self.D_range)\n",
        "        E = np.dot(Y, buildMhat(self.X.T, self.Y_range, self.D_range))\n",
        "        self.D = ODL_updateD(self.D, E, F)\n",
        "\n",
        "    def _extract_fromX1(self, X1):\n",
        "        K = self.D_range[-1]\n",
        "        return (X1[:K, :], X1[K:, :]) if self.k0 > 0 else (X1[:K, :], _zero)\n",
        "\n",
        "    def _updateXX0(self):\n",
        "        clf = _UpdateXX0(self.Y, self.Y_range, self.D, self.D_range, self.D0,\n",
        "                         self.k0, lambd=self.lambd, lambd2=self.lambd2)\n",
        "\n",
        "        X1 = np.vstack((self.X, self.X0)) if self.k0 > 0 else self.X\n",
        "        X1 = clf.solve(Xinit=X1)\n",
        "        self.X, self.X0 = self._extract_fromX1(X1)\n",
        "\n",
        "    def _buildYhat(self):\n",
        "        \"\"\"\n",
        "        Yhat = [Yhat_1, Yhat_2, ..., Yhat_C]\n",
        "        where Yhat_c = Yc - Dc*Xcc\n",
        "        \"\"\"\n",
        "        Yhat = np.zeros_like(self.Y)\n",
        "        for c in range(self.num_classes):\n",
        "            Yc = get_block_col(self.Y, c, self.Y_range)\n",
        "            Dc = get_block_col(self.D, c, self.D_range)\n",
        "            Xcc = get_block(self.X, c, c, self.D_range, self.Y_range)\n",
        "            Yhat[:, self.Y_range[c]: self.Y_range[c+1]] = Yc - np.dot(Dc, Xcc)\n",
        "        return Yhat\n",
        "\n",
        "    def _updateD0(self):\n",
        "        Ybar = self.Y - np.dot(self.D, self.X)\n",
        "        L = (Ybar + self._buildYhat())/2\n",
        "        self.D0 = min_rank_dict(L, self.X0, self.eta/2, Dinit=self.D0, iterations=50)\n",
        "\n",
        "    def _initialize(self):\n",
        "        for c in range(self.num_classes):\n",
        "            clf = ODL(lambd=self.lambd, k=self.D_range[c+1] - self.D_range[c])\n",
        "            clf.fit(self._getYc(c), iterations=5)\n",
        "            self.D[:, self.D_range[c]:self.D_range[c+1]] = clf.D\n",
        "            self.X[self.D_range[c]: self.D_range[c+1],\n",
        "                   self.Y_range[c]: self.Y_range[c+1]] = clf.X\n",
        "\n",
        "        if self.k0 > 0:\n",
        "            odl = ODL(lambd=self.lambd, k=self.k0)\n",
        "            odl.fit(self.Y)\n",
        "            self.D0 = odl.D\n",
        "            self.X0 = odl.X\n",
        "\n",
        "    def _fidelity(self):\n",
        "        \"\"\"\n",
        "        Calculating the fidelity term in FDDL\n",
        "        sum_{c=1}^C ||Y_c - D_cX^c_c||_F^2 + sum_{i != c} ||D_c X^c_i||_F^2$\n",
        "        \"\"\"\n",
        "        cost = 0\n",
        "        Y = self.Y - np.dot(self.D0, self.X0) if self.k0 > 0 else self.Y.copy()\n",
        "        for c in range(self.num_classes):\n",
        "            Yc = get_block_col(Y, c, self.Y_range)\n",
        "            Dc = get_block_col(self.D, c, self.D_range)\n",
        "            Xc = get_block_row(self.X, c, self.D_range)\n",
        "            Xcc = get_block_col(Xc, c, self.Y_range)\n",
        "            cost += normF2(Yc - np.dot(Dc, Xcc))\n",
        "            for i in range(self.num_classes):\n",
        "                if i == c:\n",
        "                    continue\n",
        "                Xci = get_block_col(Xc, i, self.Y_range)\n",
        "                cost += normF2(np.dot(Dc, Xci))\n",
        "        return cost\n",
        "\n",
        "    def _discriminative(self):\n",
        "        \"\"\"\n",
        "        * calculating the discriminative term in FDDL[[4]](#fn_fdd):\n",
        "        * $\\|X\\|_F^2 + \\sum_{c=1}^C (\\|Xc - Mc\\|_F^2 - \\|Mc - M\\|_F^2) $\n",
        "        \"\"\"\n",
        "        cost = normF2(self.X)\n",
        "        m = np.mean(self.X, axis=1)\n",
        "        for c in range(self.num_classes):\n",
        "            Xc = get_block_col(self.X, c, self.Y_range)\n",
        "            Mc = build_mean_matrix(Xc)\n",
        "            cost += normF2(Xc - Mc)\n",
        "            M = repmat(m, 1, Xc.shape[1])\n",
        "            cost -= normF2(Mc - M)\n",
        "        return cost\n",
        "\n",
        "    def loss(self):\n",
        "        Y = self.Y.copy()\n",
        "        if self.k0 > 0:\n",
        "            Y -= np.dot(self.D0, self.X0)\n",
        "        cost = 0.5*normF2(Y - np.dot(self.D, self.X)) + \\\n",
        "            0.5*self._fidelity() + \\\n",
        "            0.5*self.lambd2*self._discriminative() + \\\n",
        "            self.lambd*norm1(self.X)\n",
        "\n",
        "        if self.k0 > 0:\n",
        "            cost += self.lambd*norm1(self.X0) + \\\n",
        "                0.5*self.lambd2*normF2(self.X0 - build_mean_matrix(self.X0)) \\\n",
        "                + self.eta*nuclearnorm(self.D0)\n",
        "        return cost\n",
        "\n",
        "    def predict(self, Y):\n",
        "        N = Y.shape[1]\n",
        "        E = np.zeros((self.num_classes, N))\n",
        "        for c in range(self.num_classes):\n",
        "            # Dc in D only\n",
        "            Dc_ = get_block_col(self.D, c, self.D_range)\n",
        "            # Dc in D and D0\n",
        "            Dc = np.hstack((Dc_, self.D0)) if self.k0 > 0 else Dc_\n",
        "            print(\"Dc_sum:\",Dc_.sum())\n",
        "            print(\"sparsity\",np.count_nonzero(self.D0))\n",
        "\n",
        "            lasso = Lasso(Dc, lambd=self.lambd)\n",
        "            lasso.fit(Y)\n",
        "            Xc = lasso.solve()\n",
        "            residual_matrix = Y - np.dot(Dc, Xc)\n",
        "            E[c, :] = 0.5*np.sum(residual_matrix*residual_matrix, axis=0) +\\\n",
        "                self.lambd*np.sum(np.abs(Xc), axis=0)\n",
        "        pred = np.argmin(E, axis=0) + 1\n",
        "        return pred\n",
        "\n",
        "\n",
        "# def mini_test_unit():\n",
        "#     print('\\n================================================================')\n",
        "#     print('Mini Unit test: Low-rank shared Dictionary Learning')\n",
        "#     dataset = 'myFlower'\n",
        "#     N_train = 5\n",
        "#     Y_train, Y_test, label_train, label_test = training_test_split(dataset, N_train)\n",
        "#     lrsdl = LRSDL(lambd=0.01, lambd2=0.01, eta=0.1, k=4, k0=5)\n",
        "#     lrsdl.fit(Y_train, label_train, iterations=30, verbose=True)\n",
        "#     lrsdl.evaluate(Y_test, label_test)\n",
        "\n",
        "\n",
        "# def mini_test_unit_FDDL():\n",
        "#     print('\\n================================================================')\n",
        "#     print('Mini Unit test: Fisher Disrciminant Dicationary Learning')\n",
        "#     dataset = 'myYaleB'\n",
        "#     N_train = 5\n",
        "#     Y_train, Y_test, label_train, label_test = train_test_split(dataset, N_train)\n",
        "#     lrsdl = LRSDL(lambd=0.01, lambd2=0.01, eta=0.1, k=4, k0=0)\n",
        "#     lrsdl.fit(Y_train, label_train, iterations=30, verbose=True)\n",
        "#     lrsdl.evaluate(Y_test, label_test)\n",
        "\n",
        "\n",
        "# def test_unit_FDDL():\n",
        "#     print('\\n================================================================')\n",
        "#     print('Unit test: Fisher Disrciminant Dicationary Learning')\n",
        "#     dataset = 'myYaleB'\n",
        "#     N_train = 30\n",
        "#     Y_train, Y_test, label_train, label_test = train_test_split(dataset, N_train)\n",
        "#     lrsdl = LRSDL(lambd=0.01, lambd2=0.01, eta=0.1, k=20, k0=0)\n",
        "#     lrsdl.fit(Y_train, label_train, iterations=30, verbose=True)\n",
        "#     lrsdl.evaluate(Y_test, label_test)\n",
        "\n",
        "\n",
        "def test_unit():\n",
        "    print('\\n================================================================')\n",
        "    print('Unit test: Low-rank shared Dictionary Learning')\n",
        "    # dataset = 'myFlower'\n",
        "    # N_train = 60\n",
        "    # Y_train, Y_test, label_train, label_test = training_test_split(dataset, N_train)\n",
        "    # lrsdl = LRSDL(lambd=0.01, lambd2=0.01, eta=0.1, k=20, k0=10)\n",
        "    # lrsdl.fit(Y_train, label_train, iterations=10, verbose=True)\n",
        "    # lrsdl.evaluate(Y_test, label_test)\n",
        "    dataset = 'mnist'\n",
        "    N_train = 100\n",
        "    Y_train, Y_test, label_train, label_test = training_test_split(dataset, N_train)\n",
        "    lrsdl = LRSDL(lambd=0.05, lambd2=0.01, eta=0.001, k=20, k0=6)\n",
        "    lrsdl.fit(Y_train, label_train, iterations=40, verbose=True)\n",
        "    lrsdl.evaluate(Y_test, label_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "#     mini_test_unit_FDDL()\n",
        "  #  mini_test_unit()\n",
        "#     test_unit_FDDL()\n",
        "    test_unit()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "mat = scipy.io.loadmat('mnist-original.mat')\n",
        "mat[\"data\"].shape\n"
      ],
      "metadata": {
        "id": "2QEpqTTym4oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ece9e8-2776-4712-e2cd-541b56aec41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 70000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "matf = scipy.io.loadmat('myFlower102.mat')\n",
        "matf[\"Y_train\"].shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ru19OIpyt3c",
        "outputId": "252642b0-0952-47fa-c047-7592dbe924cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1020)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# https://www.kaggle.com/general/74235\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "zTgu4mbXygmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload();\n",
        "! mkdir ~/.kaggle\n",
        "! mv kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "LaKaufzH0TsI",
        "outputId": "5735951c-6fc6-41fc-a042-6d84758cd3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-41b4a568-2aca-4d41-b5be-2c435cbb6f46\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-41b4a568-2aca-4d41-b5be-2c435cbb6f46\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-df99b7eb3f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' mkdir ~/.kaggle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' mv kaggle.json ~/.kaggle/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m   \"\"\"\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    151\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    152\u001b[0m             output_id=output_id))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Rt5IeoDw0nzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d avnishnish/mnist-original"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx9-G7vpPCHn",
        "outputId": "6865fb05-4f7c-4680-d364-4745881e6f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/mnist-original.zip"
      ],
      "metadata": {
        "id": "pHTPO45EPsHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df394ccd-8b47-431b-89f1-7a86c0c066f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace mnist-original.mat? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_images_sample(X, Y):\n",
        "    # Draw plot for images sample\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    rand_indicies = np.random.randint(len(X), size=25)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        index = rand_indicies[i]\n",
        "        plt.imshow(np.squeeze(X[index]), cmap=plt.cm.binary)\n",
        "        plt.xlabel"
      ],
      "metadata": {
        "id": "jU3omppWv54E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw plot for images sample\n",
        "dataset = 'mnist'\n",
        "N_train = 100\n",
        "Y_train, Y_test, label_train, label_test = training_test_split(dataset, N_train)\n",
        "print(label_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bieo5IyQRrsJ",
        "outputId": "74c258eb-a1b8-4a4b-91ac-f71073a9838e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "label train (60000,)\n",
            "label train [0. 0. 0. ... 9. 9. 9.]\n",
            "range [0, 5923, 12665, 18623, 24754, 30596, 36017, 41935, 48200, 54051, 60000]\n",
            "Y_train [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Label_train [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
            "Y_test [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Label_test [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TILZT_ceamRO",
        "outputId": "7363f551-5f36-431d-8a4c-3c1b9da70cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
          ]
        }
      ]
    }
  ]
}